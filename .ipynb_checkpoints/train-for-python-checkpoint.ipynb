{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Libraries\n",
    "\n",
    "This makes use of tflearn and tensorflow. Generally speaking, better results were used when using higher level wrappers such as tflearn or slim, so we elected to use those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*.pyc\r\n",
      "*.npy\r\n"
     ]
    }
   ],
   "source": [
    "!cat .gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import pywt\n",
    "from tflearn.layers.recurrent import bidirectional_rnn, BasicLSTMCell\n",
    "from tflearn.layers.core import dropout\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from audio_dataset_generator import AudioDatasetGenerator\n",
    "from audio_dataset_generator import AudioWaveletDatasetGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Program Parameters\n",
    "\n",
    "Set the appropriate settings for the model here. Audio data path will choose which folder to get fft frames from. Make sure the files are .wav and they are the only files you want as all the files fft frames are loaded and concatenated together. \n",
    "\n",
    "Please note:\n",
    "* ```rnn_type``` should equal any of the following values: lstm, gru, bi_lstm, bi_gru\n",
    "* ```number_rnn_layers``` should be greater than 0\n",
    "* ```activation``` should be the string defined in the tflearn library for [any of the activations defined here](http://tflearn.org/activations/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "load_model           = False\n",
    "\n",
    "# Dataset\n",
    "sequence_length      = 40\n",
    "audio_data_path      = \"assets/electronic_piano/\"\n",
    "force_new_dataset    = True\n",
    "use_wavelets         = False\n",
    "predict_coeff        = False\n",
    "wavelet              = 'db10'\n",
    "\n",
    "# Feature Extraction and Audio Genreation\n",
    "sample_rate          = 22050\n",
    "fft_settings         = [2048, 1024, 512]\n",
    "fft_size             = fft_settings[0]\n",
    "window_size          = fft_settings[1]\n",
    "hop_size             = fft_settings[2]\n",
    "\n",
    "# General Network\n",
    "learning_rate        = 1e-3\n",
    "amount_epochs        = 5\n",
    "batch_size           = 64\n",
    "keep_prob            = 0.2\n",
    "loss_type            = \"mean_square\"\n",
    "activation           = 'tanh'\n",
    "optimiser            = 'adam'\n",
    "fully_connected_dim  = 1024\n",
    "\n",
    "# Recurrent Neural Network\n",
    "rnn_type             = \"lstm\"\n",
    "number_rnn_layers    = 3\n",
    "rnn_number_units     = 256\n",
    "\n",
    "# Convolutional Neural Network\n",
    "use_cnn              = True\n",
    "number_filters       = [32]\n",
    "filter_sizes         = [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Load the Dataset\n",
    "\n",
    "Take the fft magnitudes from the folder specified at the audio_data_path and create the magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% data generation complete.  (729, 40, 1025, 1)\n"
     ]
    }
   ],
   "source": [
    "if use_wavelets:\n",
    "    dataset = AudioWaveletDatasetGenerator(1024, sequence_length, \n",
    "                                    sample_rate)\n",
    "else:\n",
    "    dataset = AudioDatasetGenerator(fft_size, window_size, hop_size,\n",
    "                                sequence_length, sample_rate)\n",
    "\n",
    "dataset.load(audio_data_path, force_new_dataset)\n",
    "\n",
    "if use_cnn:\n",
    "    dataset.x_frames = dataset.x_frames.reshape(dataset.x_frames.shape[0], \n",
    "                                                dataset.x_frames.shape[1], \n",
    "                                                dataset.x_frames.shape[2], 1)\n",
    "print(dataset.x_frames.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model\n",
    "\n",
    "A couple of helper methods are defined to speed up the process of experimenting with the model's archetecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(net, filters, kernels, non_linearity):  \n",
    "    \"\"\"\n",
    "    A quick function to build a conv net. \n",
    "    At the end it reshapes the network to be 3d to work with recurrent units.\n",
    "    \"\"\"\n",
    "    assert len(filters) == len(kernels)\n",
    "    \n",
    "    for i in range(len(filters)):\n",
    "        net = conv_2d(net, filters[i], kernels[i], activation=non_linearity)\n",
    "        net = max_pool_2d(net, 2)\n",
    "        \n",
    "    dim1 = net.get_shape().as_list()[1]\n",
    "    dim2 = net.get_shape().as_list()[2]\n",
    "    dim3 = net.get_shape().as_list()[3]\n",
    "    return tf.reshape(net, [-1, dim1 * dim3, dim2])\n",
    "   \n",
    "                      \n",
    "def recurrent_net(net, rec_type, rec_size, return_sequence):\n",
    "    \"\"\"\n",
    "    A quick if else block to build a recurrent layer, based on the type specified\n",
    "    by the user.\n",
    "    \"\"\"\n",
    "    if rec_type == 'lstm':\n",
    "        net = tflearn.layers.recurrent.lstm(net, rec_size, return_seq=return_sequence)\n",
    "    elif rec_type == 'gru':\n",
    "        net = tflearn.layers.recurrent.gru(net, rec_size, return_seq=return_sequence)\n",
    "    elif rec_type == 'bi_lstm':\n",
    "        net = bidirectional_rnn(net, \n",
    "                                BasicLSTMCell(rec_size), \n",
    "                                BasicLSTMCell(rec_size), \n",
    "                                return_seq=return_sequence)\n",
    "    elif rec_type == 'bi_gru':\n",
    "        net = bidirectional_rnn(net, \n",
    "                                GRUCell(rec_size), \n",
    "                                GRUCell(rec_size), \n",
    "                                return_seq=return_sequence)\n",
    "    else:\n",
    "        raise ValueError('Incorrect rnn type passed. Try lstm, gru, bi_lstm or bi_gru.')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the actual structure of the net is specified.\n",
    "\n",
    "If use_cnn is true the model will be prefixed with a cnn. This will slow things down but produces a different kind of result. The model regardless then builds a rnn layer which runs into a fully connected layer. before being passed to linear outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/lib/python3.6/site-packages/tflearn/initializations.py:119: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "if use_cnn:\n",
    "    net = tflearn.input_data([None, \n",
    "                              dataset.x_frames.shape[1], \n",
    "                              dataset.x_frames.shape[2], \n",
    "                              dataset.x_frames.shape[3]], name=\"input_data0\")\n",
    "    net = conv_net(net, number_filters, filter_sizes, activation)\n",
    "else:                  \n",
    "    net = tflearn.input_data([None, \n",
    "                              dataset.x_frames.shape[1], \n",
    "                              dataset.x_frames.shape[2]], name=\"input_data0\") \n",
    "\n",
    "# Batch Norm\n",
    "net = tflearn.batch_normalization(net, name=\"batch_norm0\")\n",
    "  \n",
    "# Recurrent\n",
    "for layer in range(number_rnn_layers):\n",
    "    return_sequence = False if layer == (number_rnn_layers - 1) else True\n",
    "    net = recurrent_net(net, rnn_type, rnn_number_units, return_sequence)\n",
    "    net = dropout(net, 1-keep_prob) if keep_prob < 1.0 else net \n",
    "\n",
    "# Dense + MLP Out\n",
    "net = tflearn.fully_connected(net, dataset.y_frames.shape[1], \n",
    "                              activation=activation,                                            \n",
    "                              regularizer='L2', \n",
    "                              weight_decay=0.001)\n",
    "                      \n",
    "net = tflearn.fully_connected(net, dataset.y_frames.shape[1], \n",
    "                              activation='linear')\n",
    "\n",
    "net = tflearn.regression(net, optimizer=optimiser, learning_rate=learning_rate,                                 \n",
    "                         loss=loss_type)\n",
    "\n",
    "model = tflearn.DNN(net, tensorboard_verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Currently we aren't worried about overfitting, so we just pass the entire dataset of generated magnitudes. Perhaps we might want to change this, in which case we would need to split the dataset into training, validation and testing subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: 2WXP5O\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "WARNING:tensorflow:Issue encountered when serializing layer_tensor/LSTM.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'list' object has no attribute 'name'\n",
      "WARNING:tensorflow:Issue encountered when serializing layer_tensor/Dropout.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'list' object has no attribute 'name'\n",
      "WARNING:tensorflow:Issue encountered when serializing layer_tensor/LSTM_1.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'list' object has no attribute 'name'\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 729\n",
      "Validation samples: 0\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model.fit(dataset.x_frames, dataset.y_frames, show_metric=True, \n",
    "          batch_size=batch_size, n_epoch=amount_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"model.tfl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load(\"model.tfl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Audio\n",
    "\n",
    "Here we generate audio. Choose the amount of samples, then how long they should be with sequence length, and then how many iterations the griffin lim algorithm should run for. The impluse scale is something we haven't objectively tested yet, but it just scales the initial magnitudes for the models first predictions. \n",
    "\n",
    "Depending on whether there are convolutions in the network will mean we will need to reshape appropriately. Also note that the audio generated is saved in the audio variable as a 2d numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amount_samples      = 1\n",
    "sequence_length_max = 1000\n",
    "impulse_scale       = 1.0\n",
    "griffin_iterations  = 60\n",
    "random_chance       = 0.0\n",
    "random_strength     = 0.0\n",
    "\n",
    "dimension1 = dataset.x_frames.shape[1]\n",
    "dimension2 = dataset.x_frames.shape[2]\n",
    "shape = (1, dimension1, dimension2, 1) if use_cnn else (1, dimension1, dimension2)\n",
    "\n",
    "audio = []\n",
    "\n",
    "if use_wavelets:\n",
    "    temp_audio = np.array(0)\n",
    "for i in range(amount_samples):                                                                                                                                   \n",
    "    \n",
    "    random_index = np.random.randint(0, (len(dataset.x_frames) - 1))                                                                                                                    \n",
    "    print(\"impulse\", impulse.shape, impulse.reshape(shape).shape)                                                                                                                                                            \n",
    "    impulse = np.array(dataset.x_frames[random_index]) * impulse_scale\n",
    "    predicted_magnitudes = impulse\n",
    "    print(\"prediction\", prediction.shape)\n",
    "    if use_wavelets:\n",
    "        for seq in range (impulse.shape[0]):\n",
    "            coeffs = pywt.array_to_coeffs(impulse[seq], dataset.coeff_slices)\n",
    "            recon = (pywt.waverecn(coeffs, wavelet=wavelet))\n",
    "            temp_audio = np.append(temp_audio, recon)\n",
    "    for j in range(sequence_length_max):\n",
    "\n",
    "        prediction = model.predict(impulse.reshape(shape))\n",
    "        \n",
    "        #Wavelet audio\n",
    "        if use_wavelets:\n",
    "            coeffs = pywt.array_to_coeffs(prediction[0], dataset.coeff_slices)\n",
    "            recon = (pywt.waverecn(coeffs, wavelet=wavelet))\n",
    "            temp_audio = np.append(temp_audio, recon)\n",
    "        \n",
    "        if use_cnn:\n",
    "            prediction = prediction.reshape(1, dataset.y_frames.shape[1], 1)\n",
    "        \n",
    "        predicted_magnitudes = np.vstack((predicted_magnitudes, prediction))\n",
    "        print(\"predicted_magnitudes\", predicted_magnitudes.shape)\n",
    "        impulse = predicted_magnitudes[-sequence_length:]\n",
    "        \n",
    "        if (np.random.random_sample() < random_chance) :\n",
    "            idx = np.random.randint(0, dataset.sequence_length)\n",
    "            impulse[idx] = impulse[idx] + np.random.random_sample(impulse[idx].shape) * random_strength\n",
    "        \n",
    "        done = int(float(i * sequence_length_max + j) / float(amount_samples * sequence_length_max) * 100.0) + 1\n",
    "        sys.stdout.write('{}% audio generation complete.   \\r'.format(done))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    if use_wavelets:                                                                                                                                                                        \n",
    "        audio += [temp_audio]\n",
    "    else:\n",
    "        predicted_magnitudes = np.array(predicted_magnitudes).reshape(-1, window_size+1)                                                                           \n",
    "        audio += [dataset.griffin_lim(predicted_magnitudes.T, griffin_iterations)]\n",
    "audio = np.array(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Plotting the Results\n",
    "\n",
    "Select the index you want to listen to. It may be useful to plot stuff below too using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "i = 0\n",
    "Audio(audio[i], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "plt.specgram(audio[i], NFFT=2048, Fs=sample_rate, noverlap=512)\n",
    "\n",
    "# Plot a spectrogram\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#just to test the input sound\n",
    "testaudio = dataset.griffin_lim(dataset.x_frames[10].T)\n",
    "Audio(testaudio, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(dataset.x_frames[5][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(dataset.y_frames[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
